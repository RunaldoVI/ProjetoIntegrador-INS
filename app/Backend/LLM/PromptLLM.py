import requests
import json

def obter_pergunta(instrucoes_adicionais=None):
    prompt = (
        "Tens √† tua frente um excerto de um question√°rio estruturado (como o NHANES). "
        "Extrai, de forma organizada, todas as perguntas v√°lidas com base nas instru√ß√µes abaixo.\n\n"

        "‚ùó Instru√ß√µes importantes:\n"
        "- N√£o traduzas nada.\n"
        "- Cada pergunta come√ßa com um identificador √∫nico no formato 'XXX.###' (ex: 'DPQ.010', 'DUQ.200', 'ALQ.111', etc).\n"
        "- Se houver um texto introdut√≥rio que se aplica a v√°rias perguntas consecutivas (ex: 'Over the last 2 weeks...'), considera esse texto como uma 'Sec√ß√£o' e aplica-o a todas as perguntas seguintes at√© que o contexto mude.\n"
        "- Se o bloco n√£o tiver sec√ß√£o aplic√°vel, define \"Sec√ß√£o\": \"Nenhuma\".\n"
        "- Ignora qualquer bloco que seja:\n"
        "  - apenas instru√ß√µes t√©cnicas (ex: 'CHECK ITEM', 'BOX 1', 'GO TO', 'HANDCARD', 'ENTER AGE', etc.),\n"
        "  - ou uma pergunta que n√£o tenha pelo menos uma op√ß√£o de resposta com valor num√©rico (ex: 0, 1, 2, ...).\n"
        "- A pergunta pode vir imediatamente ap√≥s o identificador ou numa linha seguinte. Junta as partes corretamente.\n"
        "- As respostas devem ser listadas com o seu texto e valor num√©rico.\n\n"

        "‚úÖ Formato obrigat√≥rio de sa√≠da (em JSON):\n\n"
        "{\n"
        "  \"Identificador\": \"XXX.###\",\n"
        "  \"Sec√ß√£o\": \"Texto da sec√ß√£o ou 'Nenhuma'\",\n"
        "  \"Pergunta\": \"Texto da pergunta\",\n"
        "  \"Respostas\": [\n"
        "    { \"op√ß√£o\": \"Texto da resposta\", \"valor\": \"0\" },\n"
        "    { \"op√ß√£o\": \"Texto da resposta\", \"valor\": \"1\" },\n"
        "    ...\n"
        "  ]\n"
        "}\n\n"
        "Repete este formato JSON para cada pergunta v√°lida."
    )

    if instrucoes_adicionais:
        prompt += "\n\nüìå Instru√ß√µes adicionais do utilizador:\n" + instrucoes_adicionais.strip()

    return prompt


def enviar_pagina_para_llm(texto_pagina, prompt, modelo="mistral", url="http://localhost:11434/api/chat"):
    """
    Envia um texto para o modelo LLM via Ollama (ou servidor compat√≠vel).
    """
    payload = {
        "model": modelo,
        "messages": [
            {
                "role": "system",
                "content": "Est√°s a interpretar uma p√°gina de um question√°rio cl√≠nico estruturado. Responde apenas com base nesta p√°gina. Responde sempre no formato JSON definido."
            },
            {
                "role": "user",
                "content": f"{texto_pagina}\n\n{prompt}"
            }
        ]
    }

    resposta_total = ""
    try:
        response = requests.post(url, json=payload, stream=True)

        if response.status_code == 200:
            for line in response.iter_lines(decode_unicode=True):
                if line:
                    try:
                        data = json.loads(line)
                        if "message" in data and "content" in data["message"]:
                            resposta_total += data["message"]["content"]
                    except json.JSONDecodeError:
                        resposta_total += f"\n‚ö†Ô∏è Erro a processar linha: {line}"
        else:
            resposta_total = f"‚ùå Erro: {response.status_code}\n{response.text}"

    except Exception as e:
        resposta_total = f"‚ùå Erro de conex√£o: {e}"

    return resposta_total
